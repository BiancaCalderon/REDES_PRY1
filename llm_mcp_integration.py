import asyncio
import anthropic
from contextlib import AsyncExitStack
from mcp import ClientSession, StdioServerParameters
from mcp.client.stdio import stdio_client
from dotenv import load_dotenv
import os

load_dotenv()

class AstronomyMCPChatbot:
    def __init__(self):
        self.anthropic = anthropic.Anthropic(api_key=os.getenv("ANTHROPIC_API_KEY"))
        self.conversation = []

    async def create_file_with_mcp(self, content):
        """Create file using MCP Filesystem with proper context management"""
        try:
            fs_server = StdioServerParameters(
                command="npx",
                args=["-y", "@modelcontextprotocol/server-filesystem", "workspace"]
            )
            
            async with AsyncExitStack() as stack:
                (read, write) = await stack.enter_async_context(stdio_client(fs_server))
                fs_session = await stack.enter_async_context(ClientSession(read, write))
                await fs_session.initialize()
                
                # Create the file
                result = await fs_session.call_tool("write_file", {
                    "path": "workspace/astronomy_readme.md",
                    "content": content
                })
                
                return result
        except Exception as e:
            print(f"MCP Error: {e}")
            return None

    async def chat_with_mcp(self, message):
        """Chat with LLM and use MCP Filesystem when needed"""
        self.conversation.append({"role": "user", "content": message})

        # Check if user wants to create a file
        if "create file" in message.lower() or "crear archivo" in message.lower():
            try:
                # Ask LLM to generate content
                llm_response = self.anthropic.messages.create(
                    model="claude-sonnet-4-20250514",
                    messages=self.conversation + [{"role": "user", "content": "Generate content for a README file about astronomy topics. Make it informative and well-structured."}],
                    max_tokens=500
                )
                content = llm_response.content[0].text

                # Use MCP Filesystem to write the file
                result = await self.create_file_with_mcp(content)
                
                if result:
                    response = f"File created successfully! Content generated by Claude and written using MCP Filesystem.\n\nGenerated content:\n{content}"
                else:
                    response = f"Content generated by Claude but failed to create file with MCP.\n\nGenerated content:\n{content}"
                    
            except Exception as e:
                response = f"Error: {str(e)}"
        else:
            # Normal LLM response
            try:
                llm_response = self.anthropic.messages.create(
                    model="claude-sonnet-4-20250514",
                    messages=self.conversation,
                    max_tokens=500
                )
                response = llm_response.content[0].text
            except Exception as e:
                response = f"Error getting LLM response: {str(e)}"

        self.conversation.append({"role": "assistant", "content": response})
        return response

async def main():
    chatbot = AstronomyMCPChatbot()
    
    print("Astronomy MCP Chatbot ready!")
    print("Try saying: 'Create file README about astronomy'")
    
    # Example usage
    response = await chatbot.chat_with_mcp("Create file README about astronomy")
    print(f"Assistant: {response}")

if __name__ == "__main__":
    asyncio.run(main())